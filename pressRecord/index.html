<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Recorder</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto; padding: 20px; }
        button { margin: 5px; }
        #recordingsList { list-style-type: none; padding: 0; }
        #recordingsList li { margin-bottom: 10px; }
        #waveform { width: 100%; height: 100px; }
        #editControls { display: none; margin-top: 10px; }
        #timeRange { width: 100%; }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/wavesurfer.js/6.6.3/wavesurfer.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/wavesurfer.js/6.6.3/plugin/wavesurfer.regions.min.js"></script>
</head>
<body>
    <h1>Voice Recorder</h1>
    <button id="recordButton">Start Recording</button>
    <button id="resumeAudioContext" style="display: none;">Resume Audio</button>
    <p id="recordingStatus"></p>
    <div id="recordingInfo" style="display: none;">
        <p>Duration: <span id="duration">0:00</span></p>
        <div id="loader" style="border: 4px solid #f3f3f3; border-radius: 50%; border-top: 4px solid #3498db; width: 20px; height: 20px; animation: spin 2s linear infinite;"></div>
    </div>
    <div id="waveformContainer" style="display: none;">
        <div id="waveform"></div>
        <div id="audioInfo">
            <p>Duration: <span id="duration">0:00</span></p>
        </div>
        <div id="editControls" style="display: none;">
            <input type="range" id="timeRange" min="0" max="100" value="0">
            <button id="playPauseButton">Play/Pause</button>
            <button id="cutButton">Cut</button>
        </div>
    </div>
    <h2>Recordings</h2>
    <ul id="recordingsList"></ul>

    <script>
        let mediaRecorder;
        let audioChunks = [];
        let recordings = [];
        let startTime;
        let durationInterval;
        let isRecording = false;
        let audioContext;
        let wavesurfer;
        let currentRecordingIndex;

        const recordButton = document.getElementById('recordButton');
        const resumeAudioContextButton = document.getElementById('resumeAudioContext');
        const recordingStatus = document.getElementById('recordingStatus');
        const recordingInfo = document.getElementById('recordingInfo');
        const durationElement = document.getElementById('duration');
        const waveformContainer = document.getElementById('waveformContainer');
        const audioInfo = document.getElementById('audioInfo');
        const editControls = document.getElementById('editControls');

        async function toggleRecording() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }

            if (audioContext.state === 'suspended') {
                await audioContext.resume();
            }

            if (isRecording) {
                await stopRecording();
            } else {
                await startRecording();
            }
        }

        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);

                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    recordings.push(audioBlob);
                    audioChunks = [];
                    updateRecordingsList();
                };

                mediaRecorder.start();
                isRecording = true;
                recordButton.textContent = 'Stop Recording';
                recordingStatus.textContent = 'Recording...';
                recordingInfo.style.display = 'block';

                startTime = Date.now();
                updateDuration();
                durationInterval = setInterval(updateDuration, 1000);
            } catch (error) {
                console.error('Error starting recording:', error);
                recordingStatus.textContent = 'Error: ' + error.message;
            }
        }

        async function stopRecording() {
            mediaRecorder.stop();
            isRecording = false;
            recordButton.textContent = 'Start Recording';
            recordingStatus.textContent = 'Recording stopped.';
            clearInterval(durationInterval);
            recordingInfo.style.display = 'none';
        }

        function updateDuration() {
            const duration = Math.floor((Date.now() - startTime) / 1000);
            const minutes = Math.floor(duration / 60);
            const seconds = duration % 60;
            durationElement.textContent = `${minutes}:${seconds.toString().padStart(2, '0')}`;
        }

        function updateRecordingsList() {
            recordingsList.innerHTML = '';
            recordings.forEach((recording, index) => {
                const li = document.createElement('li');
                li.innerHTML = `
                    Recording ${index + 1}
                    <button onclick="selectRecording(${index})">Select</button>
                    <button onclick="playRecording(${index})">Play</button>
                    <button onclick="exportRecording(${index})">Export</button>
                `;
                recordingsList.appendChild(li);
            });
        }

        function selectRecording(index) {
            currentRecordingIndex = index;
            waveformContainer.style.display = 'block';
            audioInfo.style.display = 'block';
            editControls.style.display = 'block';

            if (wavesurfer) {
                wavesurfer.destroy();
            }

            wavesurfer = WaveSurfer.create({
                container: '#waveform',
                waveColor: 'violet',
                progressColor: 'purple',
                plugins: [
                    WaveSurfer.regions.create({})
                ]
            });

            wavesurfer.loadBlob(recordings[index]);

            wavesurfer.on('ready', () => {
                updateDurationFromWavesurfer();
                wavesurfer.on('audioprocess', updateDurationFromWavesurfer);

                // Add a single draggable region for selecting the portion to keep
                wavesurfer.addRegion({
                    id: 'selection',
                    start: 0,
                    end: wavesurfer.getDuration(),
                    color: 'rgba(0, 255, 0, 0.1)',
                    drag: true,
                    resize: true
                });
            });

            const timeRange = document.getElementById('timeRange');
            timeRange.addEventListener('input', () => {
                const time = wavesurfer.getDuration() * (timeRange.value / 100);
                wavesurfer.setCurrentTime(time);
            });

            wavesurfer.on('audioprocess', () => {
                timeRange.value = (wavesurfer.getCurrentTime() / wavesurfer.getDuration()) * 100;
            });

            const playPauseButton = document.getElementById('playPauseButton');
            playPauseButton.addEventListener('click', () => {
                wavesurfer.playPause();
            });

            const cutButton = document.getElementById('cutButton');
            cutButton.addEventListener('click', () => {
                const selectionRegion = wavesurfer.regions.list['selection'];
                if (selectionRegion) {
                    cutAudio(index, selectionRegion.start, selectionRegion.end);
                } else {
                    alert('Please select a valid region for cutting.');
                }
            });
        }

        function updateDurationFromWavesurfer() {
            const duration = Math.floor(wavesurfer.getCurrentTime());
            const minutes = Math.floor(duration / 60);
            const seconds = duration % 60;
            durationElement.textContent = `${minutes}:${seconds.toString().padStart(2, '0')}`;
        }

        async function cutAudio(index, start, end) {
            const audioContext = new AudioContext();
            const audioBuffer = await audioContext.decodeAudioData(await recordings[index].arrayBuffer());

            // Ensure start and end are within valid range
            start = Math.max(0, start);
            end = Math.min(audioBuffer.duration, end);
            const newDuration = end - start;
            const offlineContext = new OfflineAudioContext(
                audioBuffer.numberOfChannels,
                Math.floor(newDuration * audioBuffer.sampleRate),
                audioBuffer.sampleRate
            );

            const newBuffer = offlineContext.createBuffer(
                audioBuffer.numberOfChannels,
                Math.floor(newDuration * audioBuffer.sampleRate),
                audioBuffer.sampleRate
            );


            for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
                const channelData = audioBuffer.getChannelData(channel);
                const newChannelData = newBuffer.getChannelData(channel);

                const startOffset = Math.floor(start * audioBuffer.sampleRate);
                const endOffset = Math.floor(end * audioBuffer.sampleRate);

                if (startOffset < 0 || endOffset > channelData.length) {
                    throw new RangeError('offset is out of bounds');
                }
                newChannelData.set(channelData.subarray(startOffset, endOffset - 1));
            }

            const newBlob = await new Promise(resolve => {
                const source = offlineContext.createBufferSource();
                source.buffer = newBuffer;
                source.connect(offlineContext.destination);
                source.start();

                offlineContext.startRendering().then(renderedBuffer => {
                    resolve(bufferToWave(renderedBuffer, renderedBuffer.length));
                });
            });

            recordings[index] = newBlob;
            selectRecording(index);
        }

        function bufferToWave(abuffer, len) {
            const numOfChan = abuffer.numberOfChannels;
            const length = len * numOfChan * 2 + 44;
            const buffer = new ArrayBuffer(length);
            const view = new DataView(buffer);
            const channels = [];
            let sample;
            let offset = 0;
            let pos = 0;

            // write WAVE header
            setUint32(0x46464952);
            setUint32(length - 8);
            setUint32(0x45564157);
            setUint32(0x20746d66);
            setUint32(16);
            setUint16(1);
            setUint16(numOfChan);
            setUint32(abuffer.sampleRate);
            setUint32(abuffer.sampleRate * 2 * numOfChan);
            setUint16(numOfChan * 2);
            setUint16(16);
            setUint32(0x61746164);
            setUint32(length - pos - 4);

            for (let i = 0; i < abuffer.numberOfChannels; i++)
                channels.push(abuffer.getChannelData(i));

            while (pos < length) {
                for (let i = 0; i < numOfChan; i++) {
                    sample = Math.max(-1, Math.min(1, channels[i][offset]));
                    sample = (0.5 + sample < 0 ? sample * 32768 : sample * 32767) | 0;
                    view.setInt16(pos, sample, true);
                    pos += 2;
                }
                offset++;
            }

            return new Blob([buffer], { type: "audio/wav" });

            function setUint16(data) {
                view.setUint16(pos, data, true);
                pos += 2;
            }

            function setUint32(data) {
                view.setUint32(pos, data, true);
                pos += 4;
            }
        }

        function playRecording(index) {
            const audio = new Audio(URL.createObjectURL(recordings[index]));
            audio.play();
        }

        function exportRecording(index) {
            const url = URL.createObjectURL(recordings[index]);
            const a = document.createElement('a');
            a.style.display = 'none';
            a.href = url;
            a.download = `recording_${index + 1}.wav`;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }

        recordButton.addEventListener('click', toggleRecording);

        resumeAudioContextButton.addEventListener('click', async () => {
            if (audioContext && audioContext.state === 'suspended') {
                await audioContext.resume();
                resumeAudioContextButton.style.display = 'none';
            }
        });

        // Check AudioContext state and show resume button if needed
        function checkAudioContextState() {
            if (audioContext && audioContext.state === 'suspended') {
                resumeAudioContextButton.style.display = 'inline-block';
            } else {
                resumeAudioContextButton.style.display = 'none';
            }
        }

        // Call this function periodically or after user interactions
        setInterval(checkAudioContextState, 1000);
    </script>
</body>
</html>